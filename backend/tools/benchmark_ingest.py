import asyncio
import aiohttp
import time
import json
import uuid
import statistics
import sys
from datetime import datetime

# Target API URL (default to localhost)
API_URL = "http://localhost:8000/api/v1/ingest/logs"

# Number of total requests to send
TOTAL_LOGS = 10000 
# Number of concurrent connections/tasks
CONCURRENCY = 200

def generate_mock_log():
    return {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "level": "INFO",
        "source": "suricata",
        "message": f"Connection established to {uuid.uuid4().hex[:8]}.com",
        "ip": f"192.168.1.{uuid.uuid4().int % 255}",
        "metadata": {
            "bytes_tx": uuid.uuid4().int % 1024,
            "bytes_rx": uuid.uuid4().int % 2048
        }
    }

async def send_request(session, semaphore, url, payload, latencies):
    async with semaphore:
        start_time = time.perf_counter()
        try:
            # Setting a very short timeout since we're local and want to hammer it
            async with session.post(url, json=payload, headers={"Content-Type": "application/json"}, timeout=5) as response:
                status = response.status
                # Read response to ensure it completed
                await response.read() 
        except Exception as e:
            status = 500
        
        end_time = time.perf_counter()
        
        # Only log latency for successful queuing (HTTP 202)
        if status == 202:
            latencies.append((end_time - start_time) * 1000) # milliseconds
            
        return status

async def main():
    print(f"üöÄ AEGIS SIEM Benchmark Test")
    print(f"Targeting: {API_URL}")
    print(f"Total Logs: {TOTAL_LOGS} (Concurrency: {CONCURRENCY})")
    print("...")

    # Semaphore limits concurrent connections to prevent overloading the local OS socket limit
    semaphore = asyncio.Semaphore(CONCURRENCY)
    latencies = []
    tasks = []
    
    # Pre-generate payloads to avoid generation overhead during the test
    print("Generating payloads...")
    # We will send a large list to the batch endpoint to simulate realistic high throughput agents (like Fluentbit/Logstash)
    # The /logs endpoint accepts List[LogEntry]
    
    # Actually, to hit 5,000 EPS efficiently from a single Python script, grouping them in batches is best.
    BATCH_SIZE = 50
    batches = []
    for _ in range(TOTAL_LOGS // BATCH_SIZE):
        batch = [generate_mock_log() for _ in range(BATCH_SIZE)]
        batches.append(batch)

    num_requests = len(batches)
    print(f"Testing {num_requests} requests of size {BATCH_SIZE}...")

    # Warmup (optional)
    # Allows FastAPI and Redis connections pool to initialize
    async with aiohttp.ClientSession() as session:
        await session.post(API_URL, json=batches[0], headers={"Content-Type": "application/json"})

    start_bulk = time.perf_counter()

    async with aiohttp.ClientSession() as session:
        for batch in batches:
            task = asyncio.create_task(send_request(session, semaphore, API_URL, batch, latencies))
            tasks.append(task)
            
        # Wait for all requests to finish
        results = await asyncio.gather(*tasks)

    end_bulk = time.perf_counter()
    duration = end_bulk - start_bulk

    success_count = results.count(202) * BATCH_SIZE
    failed_count = (len(results) - results.count(202)) * BATCH_SIZE
    
    eps = success_count / duration

    print("\n" + "="*40)
    print(f"üèÅ BENCHMARK RESULTS")
    print("="*40)
    print(f"Total Time Taken:     {duration:.3f} seconds")
    print(f"Logs Processed:       {success_count} / {TOTAL_LOGS}")
    if failed_count > 0:
        print(f"Failed/Dropped:       {failed_count}")
    
    print("-" * 40)
    print(f"üî• Throughput (EPS):  {eps:,.0f} Events/Second")
    print("-" * 40)
    
    if latencies:
        avg_rt = statistics.mean(latencies)
        p95_rt = statistics.quantiles(latencies, n=100)[94] if len(latencies) > 100 else max(latencies)
        # Note: Response time here is the time to queue, not full processing time.
        # But this fits the "buffered ingestion" narrative. (Actual SOAR response under 100ms happens async).
        print(f"Average API Latency:  {avg_rt:.2f} ms")
        print(f"P95 API Latency:      {p95_rt:.2f} ms")
    
    print("="*40)
    
    if eps >= 5000:
        print("‚úÖ SUCCESS: The system successfully demonstrated >5,000 EPS.")
    else:
        print("‚ö†Ô∏è NOTE: Did not hit 5,000 EPS. This may be limited by local hardware or Python's asyncio overhead.")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
